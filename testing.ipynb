{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nati\\Desktop\\Implementations\\ImplementationsVenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sktime.datasets import load_from_ucr_tsv_to_dataframe\n",
    "from sktime.datasets import load_from_tsfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utilities_helper import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sktime.datasets import load_from_ucr_tsv_to_dataframe, load_from_tsfile\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current working directory\n",
    "current_dir = os.getcwd()\n",
    "# get one directory backwards but dont change dir\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "# Datasets dir\n",
    "original_datasets_dir = os.path.join(parent_dir,\"Datasets\")\n",
    "# Synthetic datasets dir\n",
    "synthetic_datasets_dir = os.path.join(current_dir,\"DGAN_data\")\n",
    "results_dir = os.path.join(current_dir,\"pretrained_finetuned\")\n",
    "\n",
    "\n",
    "csv_path = os.path.join(current_dir,\"all_experiments_param.csv\")\n",
    "config_path = os.path.join(current_dir,\"config.yaml\")\n",
    "\n",
    "config_data = read_yaml_config(config_path)\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "csv_data = csv_data[csv_data['dataset_name'] != 'DuckDuckGeese']\n",
    "\n",
    "completed_experiments = pd.read_csv(os.path.join(current_dir,\"completed_experiments.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>BM_batch_size_ratio</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>synthetic_num_samples</th>\n",
       "      <th>dgan_original_data_ratio</th>\n",
       "      <th>finetuning_original_data_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ArticularyWordRecognition</td>\n",
       "      <td>inceptionTime</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ArticularyWordRecognition</td>\n",
       "      <td>inceptionTime</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ArticularyWordRecognition</td>\n",
       "      <td>inceptionTime</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ArticularyWordRecognition</td>\n",
       "      <td>inceptionTime</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ArticularyWordRecognition</td>\n",
       "      <td>inceptionTime</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>PenDigits</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>PenDigits</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>PenDigits</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>PenDigits</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>PenDigits</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4560 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dataset_name     model_name  BM_batch_size_ratio  \\\n",
       "0     ArticularyWordRecognition  inceptionTime                  0.1   \n",
       "1     ArticularyWordRecognition  inceptionTime                  0.1   \n",
       "2     ArticularyWordRecognition  inceptionTime                  0.1   \n",
       "3     ArticularyWordRecognition  inceptionTime                  0.1   \n",
       "4     ArticularyWordRecognition  inceptionTime                  0.1   \n",
       "...                         ...            ...                  ...   \n",
       "4795                  PenDigits           LSTM                  0.2   \n",
       "4796                  PenDigits           LSTM                  0.2   \n",
       "4797                  PenDigits           LSTM                  0.2   \n",
       "4798                  PenDigits           LSTM                  0.2   \n",
       "4799                  PenDigits           LSTM                  0.2   \n",
       "\n",
       "      hidden_dim  num_layers  epochs  learning_rate  synthetic_num_samples  \\\n",
       "0            NaN         NaN      10         0.0010                      2   \n",
       "1            NaN         NaN      10         0.0010                      2   \n",
       "2            NaN         NaN      10         0.0010                      2   \n",
       "3            NaN         NaN      10         0.0010                      4   \n",
       "4            NaN         NaN      10         0.0010                      4   \n",
       "...          ...         ...     ...            ...                    ...   \n",
       "4795       128.0         3.0      20         0.0001                      2   \n",
       "4796       128.0         3.0      20         0.0001                      2   \n",
       "4797       128.0         3.0      20         0.0001                      4   \n",
       "4798       128.0         3.0      20         0.0001                      4   \n",
       "4799       128.0         3.0      20         0.0001                      4   \n",
       "\n",
       "      dgan_original_data_ratio  finetuning_original_data_ratio  \n",
       "0                          0.7                             0.7  \n",
       "1                          0.7                             0.9  \n",
       "2                          0.9                             0.9  \n",
       "3                          0.7                             0.7  \n",
       "4                          0.7                             0.9  \n",
       "...                        ...                             ...  \n",
       "4795                       0.7                             0.9  \n",
       "4796                       0.9                             0.9  \n",
       "4797                       0.7                             0.7  \n",
       "4798                       0.7                             0.9  \n",
       "4799                       0.9                             0.9  \n",
       "\n",
       "[4560 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArticularyWordRecognition\n",
      "Series Length: 144\n",
      "Dimensions: 9\n",
      "Unique labels: ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0', '10.0', '11.0', '12.0', '13.0', '14.0', '15.0', '16.0', '17.0', '18.0', '19.0', '20.0', '21.0', '22.0', '23.0', '24.0', '25.0']\n",
      "Dataloaders created!\n",
      "Pretraining : 10 epochs\n",
      "https://app.neptune.ai/astarteam/FinalProject/e/FIN-1238\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "loss=3.311, 0 / 1100\n",
      "train accuracy = 0.31272727272727274, val_loss = 2.629415\n",
      "loss=1.968, 0 / 1100\n",
      "train accuracy = 0.8263636363636364, val_loss = 1.654886\n",
      "loss=1.194, 0 / 1100\n",
      "train accuracy = 0.9581818181818181, val_loss = 0.910528\n",
      "loss=0.579, 0 / 1100\n",
      "train accuracy = 0.9927272727272727, val_loss = 0.410328\n",
      "loss=0.241, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.176759\n",
      "loss=0.103, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.077391\n",
      "loss=0.059, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.043283\n",
      "loss=0.029, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.026139\n",
      "loss=0.022, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.018548\n",
      "loss=0.015, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.014053\n",
      "Finished Pre-training\n",
      "Pretrained model is loaded succesfully\n",
      "Finetuning : 10 epochs\n",
      "loss=2.198, 0 / 192\n",
      "train accuracy = 0.4322916666666667, val_loss = 1.876952\n",
      "validation accuracy = 0.49333333333333335, val_loss = 1.945120\n",
      "loss=1.026, 0 / 192\n",
      "train accuracy = 0.796875, val_loss = 1.018614\n",
      "validation accuracy = 0.4, val_loss = 2.200377\n",
      "loss=0.649, 0 / 192\n",
      "train accuracy = 0.9427083333333334, val_loss = 0.618835\n",
      "validation accuracy = 0.39666666666666667, val_loss = 2.185574\n",
      "loss=0.420, 0 / 192\n",
      "train accuracy = 0.984375, val_loss = 0.433984\n",
      "validation accuracy = 0.5133333333333333, val_loss = 1.757554\n",
      "loss=0.312, 0 / 192\n",
      "train accuracy = 0.9947916666666666, val_loss = 0.314164\n",
      "validation accuracy = 0.5566666666666666, val_loss = 1.457531\n",
      "loss=0.229, 0 / 192\n",
      "train accuracy = 1.0, val_loss = 0.244503\n",
      "validation accuracy = 0.6433333333333333, val_loss = 1.222711\n",
      "loss=0.186, 0 / 192\n",
      "train accuracy = 1.0, val_loss = 0.178637\n",
      "validation accuracy = 0.6766666666666666, val_loss = 1.052702\n",
      "loss=0.139, 0 / 192\n",
      "train accuracy = 1.0, val_loss = 0.133248\n",
      "validation accuracy = 0.77, val_loss = 0.763162\n",
      "loss=0.105, 0 / 192\n",
      "train accuracy = 1.0, val_loss = 0.103736\n",
      "validation accuracy = 0.8433333333333334, val_loss = 0.578689\n",
      "loss=0.084, 0 / 192\n",
      "train accuracy = 1.0, val_loss = 0.087472\n",
      "validation accuracy = 0.8733333333333333, val_loss = 0.474658\n",
      "Finished Training and validation, now uploading to Neptune.\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 43 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 43 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/astarteam/FinalProject/e/FIN-1238\n",
      "ArticularyWordRecognition\n",
      "Series Length: 144\n",
      "Dimensions: 9\n",
      "Unique labels: ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0', '10.0', '11.0', '12.0', '13.0', '14.0', '15.0', '16.0', '17.0', '18.0', '19.0', '20.0', '21.0', '22.0', '23.0', '24.0', '25.0']\n",
      "Dataloaders created!\n",
      "Pretraining : 10 epochs\n",
      "https://app.neptune.ai/astarteam/FinalProject/e/FIN-1239\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "loss=3.455, 0 / 1100\n",
      "train accuracy = 0.20727272727272728, val_loss = 2.780157\n",
      "loss=2.184, 0 / 1100\n",
      "train accuracy = 0.71, val_loss = 1.849365\n",
      "loss=1.349, 0 / 1100\n",
      "train accuracy = 0.9145454545454546, val_loss = 1.119736\n",
      "loss=0.716, 0 / 1100\n",
      "train accuracy = 0.9881818181818182, val_loss = 0.561329\n",
      "loss=0.364, 0 / 1100\n",
      "train accuracy = 0.9990909090909091, val_loss = 0.243254\n",
      "loss=0.146, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.115842\n",
      "loss=0.071, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.059311\n",
      "loss=0.042, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.039007\n",
      "loss=0.038, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.028374\n",
      "loss=0.023, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.019790\n",
      "Finished Pre-training\n",
      "Pretrained model is loaded succesfully\n",
      "Finetuning : 10 epochs\n",
      "loss=2.220, 0 / 247\n",
      "train accuracy = 0.46963562753036436, val_loss = 1.848982\n",
      "validation accuracy = 0.62, val_loss = 1.495986\n",
      "loss=1.076, 0 / 247\n",
      "train accuracy = 0.8299595141700404, val_loss = 0.948695\n",
      "validation accuracy = 0.66, val_loss = 1.350512\n",
      "loss=0.645, 0 / 247\n",
      "train accuracy = 0.9392712550607287, val_loss = 0.680721\n",
      "validation accuracy = 0.6866666666666666, val_loss = 1.110662\n",
      "loss=0.426, 0 / 247\n",
      "train accuracy = 0.979757085020243, val_loss = 0.489869\n",
      "validation accuracy = 0.7433333333333333, val_loss = 0.909745\n",
      "loss=0.386, 0 / 247\n",
      "train accuracy = 0.9878542510121457, val_loss = 0.349969\n",
      "validation accuracy = 0.78, val_loss = 0.734319\n",
      "loss=0.247, 0 / 247\n",
      "train accuracy = 0.9959514170040485, val_loss = 0.289130\n",
      "validation accuracy = 0.88, val_loss = 0.485270\n",
      "loss=0.177, 0 / 247\n",
      "train accuracy = 0.9959514170040485, val_loss = 0.203436\n",
      "validation accuracy = 0.9233333333333333, val_loss = 0.411555\n",
      "loss=0.148, 0 / 247\n",
      "train accuracy = 0.9959514170040485, val_loss = 0.158060\n",
      "validation accuracy = 0.9066666666666666, val_loss = 0.466268\n",
      "loss=0.105, 0 / 247\n",
      "train accuracy = 1.0, val_loss = 0.130961\n",
      "validation accuracy = 0.89, val_loss = 0.511155\n",
      "loss=0.104, 0 / 247\n",
      "train accuracy = 1.0, val_loss = 0.103848\n",
      "validation accuracy = 0.9266666666666666, val_loss = 0.385374\n",
      "Finished Training and validation, now uploading to Neptune.\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 23 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 23 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/astarteam/FinalProject/e/FIN-1239\n",
      "ArticularyWordRecognition\n",
      "Series Length: 144\n",
      "Dimensions: 9\n",
      "Unique labels: ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0', '10.0', '11.0', '12.0', '13.0', '14.0', '15.0', '16.0', '17.0', '18.0', '19.0', '20.0', '21.0', '22.0', '23.0', '24.0', '25.0']\n",
      "Dataloaders created!\n",
      "Pretraining : 10 epochs\n",
      "https://app.neptune.ai/astarteam/FinalProject/e/FIN-1240\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "loss=3.260, 0 / 1100\n",
      "train accuracy = 0.38272727272727275, val_loss = 2.613103\n",
      "loss=2.060, 0 / 1100\n",
      "train accuracy = 0.8709090909090909, val_loss = 1.590655\n",
      "loss=1.033, 0 / 1100\n",
      "train accuracy = 0.9881818181818182, val_loss = 0.765058\n",
      "loss=0.397, 0 / 1100\n",
      "train accuracy = 0.9981818181818182, val_loss = 0.301494\n",
      "loss=0.165, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.117353\n",
      "loss=0.076, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.054257\n",
      "loss=0.037, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.030561\n",
      "loss=0.023, 0 / 1100\n",
      "train accuracy = 1.0, val_loss = 0.020909\n",
      "loss=0.016, 0 / 1100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 114\u001b[0m\n\u001b[0;32m    104\u001b[0m run_param \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m:config_data[\u001b[39m'\u001b[39m\u001b[39mpretraining\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    105\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpatience\u001b[39m\u001b[39m\"\u001b[39m:config_data[\u001b[39m'\u001b[39m\u001b[39mpretraining\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: config_data[\u001b[39m'\u001b[39m\u001b[39mpretraining\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: config_data[\u001b[39m'\u001b[39m\u001b[39mpretraining\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m\"\u001b[39m:config_data[\u001b[39m'\u001b[39m\u001b[39mpretraining\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    109\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m: config_data[\u001b[39m'\u001b[39m\u001b[39mpretraining\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m    111\u001b[0m experiment_param \u001b[39m=\u001b[39m create_experiment_param_new(config_data)\n\u001b[1;32m--> 114\u001b[0m model_path \u001b[39m=\u001b[39m pretrain_and_finetune(synthetic_dataloader \u001b[39m=\u001b[39;49m synthetic_dataloader,\n\u001b[0;32m    115\u001b[0m                                     train_dataloader \u001b[39m=\u001b[39;49m train_dataloader,\n\u001b[0;32m    116\u001b[0m                                     validation_dataloader \u001b[39m=\u001b[39;49m validation_dataloader,\n\u001b[0;32m    117\u001b[0m                                     model \u001b[39m=\u001b[39;49m model,\n\u001b[0;32m    118\u001b[0m                                     device \u001b[39m=\u001b[39;49m device,\n\u001b[0;32m    119\u001b[0m                                     criterion \u001b[39m=\u001b[39;49m criterion,\n\u001b[0;32m    120\u001b[0m                                     optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[0;32m    121\u001b[0m                                     run_param \u001b[39m=\u001b[39;49m run_param,\n\u001b[0;32m    122\u001b[0m                                     experiment_param \u001b[39m=\u001b[39;49m experiment_param)\n\u001b[0;32m    123\u001b[0m \u001b[39m# After your experiment ends and you want to remove the current row from the csv_data\u001b[39;00m\n\u001b[0;32m    124\u001b[0m completed_experiments \u001b[39m=\u001b[39m completed_experiments\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mexperiment_index\u001b[39m\u001b[39m'\u001b[39m: index}, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\nati\\Desktop\\Implementations\\FinalProject\\FinalProject\\utilities_helper.py:705\u001b[0m, in \u001b[0;36mpretrain_and_finetune\u001b[1;34m(synthetic_dataloader, train_dataloader, validation_dataloader, model, device, criterion, optimizer, run_param, experiment_param)\u001b[0m\n\u001b[0;32m    702\u001b[0m best_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[0;32m    703\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(run_param[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m    704\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m--> 705\u001b[0m \ttrain_loss, train_acc, train_f1_score, train_precision, train_recall, mcc_train, cohen_kappa_train \u001b[39m=\u001b[39m train_loop(data_loader \u001b[39m=\u001b[39;49m synthetic_dataloader, model\u001b[39m=\u001b[39;49mmodel, device \u001b[39m=\u001b[39;49m device, loss_fn \u001b[39m=\u001b[39;49m criterion, optimizer \u001b[39m=\u001b[39;49m optimizer)\n\u001b[0;32m    706\u001b[0m \trun[\u001b[39m\"\u001b[39m\u001b[39mpretrain/accuracy\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlog(train_acc)\n\u001b[0;32m    707\u001b[0m \trun[\u001b[39m\"\u001b[39m\u001b[39mpretrain/loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlog(train_loss)\n",
      "File \u001b[1;32mc:\\Users\\nati\\Desktop\\Implementations\\FinalProject\\FinalProject\\utilities_helper.py:620\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(data_loader, model, device, loss_fn, optimizer, print_every_n)\u001b[0m\n\u001b[0;32m    618\u001b[0m cohen_kappa \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cohen_kappa_score(y, pred)\n\u001b[0;32m    619\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 620\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    621\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    622\u001b[0m loss, current \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem(), batch\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(X)\n",
      "File \u001b[1;32mc:\\Users\\nati\\Desktop\\Implementations\\ImplementationsVenv\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nati\\Desktop\\Implementations\\ImplementationsVenv\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in csv_data.iterrows():\n",
    "    if str(index) not in completed_experiments['experiment_index'].tolist():\n",
    "        # Read dataset and save into corrsponding variables\n",
    "        dataset_name = row['dataset_name']\n",
    "        dgan_original_data_ratio = row['dgan_original_data_ratio']\n",
    "        epochs = row['epochs']\n",
    "        finetuning_original_data_ratio = row['finetuning_original_data_ratio']\n",
    "\n",
    "\n",
    "        print(dataset_name)\n",
    "        # Read Synthetic data\n",
    "        pretrain_path = f'''{synthetic_datasets_dir}\\{dataset_name}\\{dgan_original_data_ratio}\\generated_data.npy'''\n",
    "        #Read Original data + metadata\n",
    "        train_path = f'''{original_datasets_dir}\\{dataset_name}\\{dataset_name}_TRAIN.ts'''\n",
    "        test_path = f'''{original_datasets_dir}\\{dataset_name}\\{dataset_name}_TEST.ts'''\n",
    "        # Loading synthetic data\n",
    "        pretraining_data = np.load(pretrain_path,allow_pickle=True)\n",
    "        X = pretraining_data.item().get('X')\n",
    "        y = pretraining_data.item().get('y')\n",
    "        # Create parameters for training\n",
    "        num_samples = X.shape[0]\n",
    "        batch_size = int(row['BM_batch_size_ratio']*num_samples)\n",
    "        #Reading metadata\n",
    "        series_length,features_num,num_classes = extract_metadata(train_path)\n",
    "        # Loading original data + metadata + preprocessing\n",
    "        X_train,y_train = load_from_tsfile(train_path)\n",
    "        X_test,y_test = load_from_tsfile(test_path)\n",
    "        X_train = preprocess_dgan(X_train,series_length)\n",
    "        X_test = preprocess_dgan(X_test,series_length)\n",
    "        _,__,y_test = map_label_int(y_test)\n",
    "        _,__,y_train = map_label_int(y_train)\n",
    "        _,__,y = map_label_int(y)\n",
    "\n",
    "        # y is a list of float string i want to transform it to integers without map_label_int\n",
    "        # y = np.array([int(float(label)) for label in y])\n",
    "        # y -= 1\n",
    "        try:\n",
    "            ssf = StratifiedShuffleSplit(n_splits=1, test_size=1-finetuning_original_data_ratio)\n",
    "            train_ind, test_ind = next(ssf.split(X_train,y_train))\n",
    "            X_train = X_train[train_ind]\n",
    "            y_train = y_train[train_ind]\n",
    "        except:\n",
    "            print(\"StratifiedShuffleSplit failed\")\n",
    "            completed_experiments = completed_experiments.append({'experiment_index': \"StratifiedShuffleSplit failed\"}, ignore_index=True)\n",
    "            completed_experiments.to_csv('completed_experiments.csv', index=False)\n",
    "\n",
    "\n",
    "        train_samples = X.shape[0]\n",
    "        test_samples = X_test.shape[0]\n",
    "        lr = row['learning_rate']\n",
    "        model_type = row['model_name']\n",
    "        #Writig to Yaml\n",
    "            # Experiment Params\n",
    "        config_data['experiment_params']['experiment_index'] = index\n",
    "        config_data['experiment_params']['dataset_name'] = dataset_name\n",
    "        config_data['experiment_params']['num_classes'] = num_classes\n",
    "        config_data['experiment_params']['num_features'] = features_num\n",
    "        config_data['experiment_params']['sequence_length'] = series_length\n",
    "        config_data['datageneration']['percentage_of_original_data'] = row['dgan_original_data_ratio']\n",
    "        # Pretraining\n",
    "        config_data['pretraining']['model_type'] = model_type\n",
    "        if model_type == \"LSTM\":\n",
    "            config_data['pretraining']['hidden_size'] = int(row['hidden_dim'])\n",
    "            config_data['pretraining']['num_layers_layers_stacked'] = int(row['num_layers'])\n",
    "        else:\n",
    "            config_data['pretraining']['hidden_size'] = 'NaN'\n",
    "            config_data['pretraining']['num_layers_layers_stacked'] = 'NaN'\n",
    "\n",
    "        config_data['pretraining']['batch_size'] = batch_size\n",
    "        config_data['pretraining']['epochs'] = epochs\n",
    "        config_data['pretraining']['learning_rate'] = lr\n",
    "        config_data['pretraining']['shuffle'] = True\n",
    "        config_data['pretraining']['save_each_epoch'] =False\n",
    "        config_data['pretraining']['optimizer'] = 'Adam'\n",
    "        config_data['pretraining']['criterion'] = \"crossentropy\"\n",
    "        #Creating a dataloader\n",
    "        synthetic_dataloader = DataLoader(TimeSeriesDataset(X,y),batch_size=batch_size,shuffle=config_data['finetuning']['shuffle'])\n",
    "        train_dataloader = DataLoader(TimeSeriesDataset(X_train,y_train),batch_size=batch_size,shuffle=config_data['finetuning']['shuffle'])\n",
    "        validation_dataloader = DataLoader(TimeSeriesDataset(X_test,y_test),batch_size=batch_size,shuffle=config_data['finetuning']['shuffle'])\n",
    "        print('Dataloaders created!')\n",
    "\n",
    "        if config_data['finetuning']['criterion'].lower() == \"crossentropy\":\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "        elif config_data['finetuning']['criterion'].lower() == \"bce\":\n",
    "                criterion = nn.BCELoss()\n",
    "\n",
    "        model = create_model_based_on_config(row['model_name'],config_data)\n",
    "        model.to(device)\n",
    "\n",
    "        # Optimizer\n",
    "        if config_data['pretraining']['optimizer'].lower() == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        elif config_data['pretraining']['optimizer'].lower() == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        elif config_data['pretraining']['optimizer'].lower() == 'rmsprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        # Pretraining\n",
    "        if config_data['pretraining']['save_each_epoch'] == True:\n",
    "            save_each_epoch = True\n",
    "        else:\n",
    "            save_each_epoch = False\n",
    "\n",
    "        run_param = {\"epochs\":config_data['pretraining']['epochs'],\n",
    "                \"patience\":config_data['pretraining']['epochs'],\n",
    "                \"batch_size\": config_data['pretraining']['batch_size'],\n",
    "                \"learning_rate\": config_data['pretraining']['learning_rate'], \n",
    "                \"criterion\":config_data['pretraining']['criterion'],\n",
    "                \"optimizer\": config_data['pretraining']['optimizer']}\n",
    "        \n",
    "        experiment_param = create_experiment_param_new(config_data)\n",
    "\n",
    "\n",
    "        model_path = pretrain_and_finetune(synthetic_dataloader = synthetic_dataloader,\n",
    "                                            train_dataloader = train_dataloader,\n",
    "                                            validation_dataloader = validation_dataloader,\n",
    "                                            model = model,\n",
    "                                            device = device,\n",
    "                                            criterion = criterion,\n",
    "                                            optimizer = optimizer,\n",
    "                                            run_param = run_param,\n",
    "                                            experiment_param = experiment_param)\n",
    "        # After your experiment ends and you want to remove the current row from the csv_data\n",
    "        completed_experiments = completed_experiments.append({'experiment_index': index}, ignore_index=True)\n",
    "\n",
    "        # Overwrite the csv file\n",
    "        completed_experiments.to_csv('completed_experiments.csv', index=False)\n",
    "    else:\n",
    "         print(\"Already did this experiment !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImplementationsVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
